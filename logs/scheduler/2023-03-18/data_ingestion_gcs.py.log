[2023-03-18 02:32:55,002] {processor.py:153} INFO - Started process (PID=180) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:32:55,004] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:32:55,005] {logging_mixin.py:115} INFO - [2023-03-18 02:32:55,005] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:32:55,161] {logging_mixin.py:115} INFO - [2023-03-18 02:32:55,156] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:32:55,162] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:32:55,185] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.186 seconds
[2023-03-18 02:33:25,300] {processor.py:153} INFO - Started process (PID=186) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:25,302] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:33:25,304] {logging_mixin.py:115} INFO - [2023-03-18 02:33:25,304] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:25,439] {logging_mixin.py:115} INFO - [2023-03-18 02:33:25,434] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:33:25,442] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:25,480] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.184 seconds
[2023-03-18 02:33:55,577] {processor.py:153} INFO - Started process (PID=191) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:55,578] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:33:55,579] {logging_mixin.py:115} INFO - [2023-03-18 02:33:55,579] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:55,670] {logging_mixin.py:115} INFO - [2023-03-18 02:33:55,665] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:33:55,671] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:55,693] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.121 seconds
[2023-03-18 02:34:25,790] {processor.py:153} INFO - Started process (PID=196) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:25,792] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:34:25,794] {logging_mixin.py:115} INFO - [2023-03-18 02:34:25,793] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:25,907] {logging_mixin.py:115} INFO - [2023-03-18 02:34:25,902] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:34:25,908] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:25,933] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.147 seconds
[2023-03-18 02:34:56,032] {processor.py:153} INFO - Started process (PID=201) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:56,034] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:34:56,035] {logging_mixin.py:115} INFO - [2023-03-18 02:34:56,035] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:56,118] {logging_mixin.py:115} INFO - [2023-03-18 02:34:56,114] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:34:56,120] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:56,140] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.113 seconds
[2023-03-18 02:35:26,231] {processor.py:153} INFO - Started process (PID=206) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:26,233] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:35:26,235] {logging_mixin.py:115} INFO - [2023-03-18 02:35:26,234] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:26,321] {logging_mixin.py:115} INFO - [2023-03-18 02:35:26,316] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:35:26,322] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:26,345] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.118 seconds
[2023-03-18 02:35:56,433] {processor.py:153} INFO - Started process (PID=211) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:56,435] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:35:56,436] {logging_mixin.py:115} INFO - [2023-03-18 02:35:56,436] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:56,524] {logging_mixin.py:115} INFO - [2023-03-18 02:35:56,518] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:35:56,525] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:56,546] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.117 seconds
[2023-03-18 02:36:26,635] {processor.py:153} INFO - Started process (PID=216) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:26,636] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:36:26,637] {logging_mixin.py:115} INFO - [2023-03-18 02:36:26,637] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:26,755] {logging_mixin.py:115} INFO - [2023-03-18 02:36:26,746] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:36:26,759] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:26,793] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.162 seconds
[2023-03-18 02:36:56,896] {processor.py:153} INFO - Started process (PID=221) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:56,898] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:36:56,900] {logging_mixin.py:115} INFO - [2023-03-18 02:36:56,900] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:56,989] {logging_mixin.py:115} INFO - [2023-03-18 02:36:56,984] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:36:56,990] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:57,014] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.122 seconds
[2023-03-18 02:37:27,105] {processor.py:153} INFO - Started process (PID=226) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:27,106] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:37:27,108] {logging_mixin.py:115} INFO - [2023-03-18 02:37:27,108] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:27,188] {logging_mixin.py:115} INFO - [2023-03-18 02:37:27,182] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:37:27,190] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:27,215] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 02:37:57,309] {processor.py:153} INFO - Started process (PID=231) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:57,311] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:37:57,312] {logging_mixin.py:115} INFO - [2023-03-18 02:37:57,312] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:57,392] {logging_mixin.py:115} INFO - [2023-03-18 02:37:57,379] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:37:57,394] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:57,415] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:38:27,510] {processor.py:153} INFO - Started process (PID=236) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:27,511] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:38:27,513] {logging_mixin.py:115} INFO - [2023-03-18 02:38:27,513] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:27,602] {logging_mixin.py:115} INFO - [2023-03-18 02:38:27,597] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:38:27,605] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:27,628] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.122 seconds
[2023-03-18 02:38:57,720] {processor.py:153} INFO - Started process (PID=241) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:57,722] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:38:57,723] {logging_mixin.py:115} INFO - [2023-03-18 02:38:57,723] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:57,809] {logging_mixin.py:115} INFO - [2023-03-18 02:38:57,804] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:38:57,811] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:57,833] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.117 seconds
[2023-03-18 02:39:27,920] {processor.py:153} INFO - Started process (PID=246) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:27,921] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:39:27,923] {logging_mixin.py:115} INFO - [2023-03-18 02:39:27,923] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:28,012] {logging_mixin.py:115} INFO - [2023-03-18 02:39:28,007] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:39:28,013] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:28,035] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.119 seconds
[2023-03-18 02:39:58,128] {processor.py:153} INFO - Started process (PID=251) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:58,130] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:39:58,131] {logging_mixin.py:115} INFO - [2023-03-18 02:39:58,131] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:58,211] {logging_mixin.py:115} INFO - [2023-03-18 02:39:58,205] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:39:58,213] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:58,233] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:40:28,322] {processor.py:153} INFO - Started process (PID=256) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:28,325] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:40:28,327] {logging_mixin.py:115} INFO - [2023-03-18 02:40:28,327] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:28,407] {logging_mixin.py:115} INFO - [2023-03-18 02:40:28,401] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:40:28,408] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:28,428] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.110 seconds
[2023-03-18 02:40:58,520] {processor.py:153} INFO - Started process (PID=261) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:58,522] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:40:58,523] {logging_mixin.py:115} INFO - [2023-03-18 02:40:58,523] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:58,596] {logging_mixin.py:115} INFO - [2023-03-18 02:40:58,592] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:40:58,597] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:58,618] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.103 seconds
[2023-03-18 02:41:28,717] {processor.py:153} INFO - Started process (PID=266) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:28,719] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:41:28,720] {logging_mixin.py:115} INFO - [2023-03-18 02:41:28,720] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:28,807] {logging_mixin.py:115} INFO - [2023-03-18 02:41:28,802] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:41:28,808] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:28,829] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.117 seconds
[2023-03-18 02:41:58,921] {processor.py:153} INFO - Started process (PID=271) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:58,923] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:41:58,925] {logging_mixin.py:115} INFO - [2023-03-18 02:41:58,925] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:59,007] {logging_mixin.py:115} INFO - [2023-03-18 02:41:59,003] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:41:59,009] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:59,033] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 02:42:29,128] {processor.py:153} INFO - Started process (PID=276) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:29,130] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:42:29,131] {logging_mixin.py:115} INFO - [2023-03-18 02:42:29,131] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:29,231] {logging_mixin.py:115} INFO - [2023-03-18 02:42:29,227] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:42:29,233] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:29,256] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.133 seconds
[2023-03-18 02:42:59,356] {processor.py:153} INFO - Started process (PID=281) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:59,357] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:42:59,359] {logging_mixin.py:115} INFO - [2023-03-18 02:42:59,358] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:59,439] {logging_mixin.py:115} INFO - [2023-03-18 02:42:59,433] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:42:59,440] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:59,460] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.108 seconds
[2023-03-18 02:43:29,557] {processor.py:153} INFO - Started process (PID=286) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:29,559] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:43:29,561] {logging_mixin.py:115} INFO - [2023-03-18 02:43:29,561] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:29,673] {logging_mixin.py:115} INFO - [2023-03-18 02:43:29,668] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:43:29,675] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:29,697] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.145 seconds
[2023-03-18 02:43:59,796] {processor.py:153} INFO - Started process (PID=291) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:59,798] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:43:59,799] {logging_mixin.py:115} INFO - [2023-03-18 02:43:59,799] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:59,919] {logging_mixin.py:115} INFO - [2023-03-18 02:43:59,913] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:43:59,921] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:59,951] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.163 seconds
[2023-03-18 02:44:30,053] {processor.py:153} INFO - Started process (PID=296) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:44:30,055] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:44:30,056] {logging_mixin.py:115} INFO - [2023-03-18 02:44:30,056] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:44:30,141] {logging_mixin.py:115} INFO - [2023-03-18 02:44:30,136] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:44:30,142] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:44:30,163] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 02:45:00,259] {processor.py:153} INFO - Started process (PID=301) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:00,260] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:45:00,262] {logging_mixin.py:115} INFO - [2023-03-18 02:45:00,262] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:00,349] {logging_mixin.py:115} INFO - [2023-03-18 02:45:00,344] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:45:00,351] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:00,374] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.120 seconds
[2023-03-18 02:45:30,469] {processor.py:153} INFO - Started process (PID=306) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:30,471] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:45:30,472] {logging_mixin.py:115} INFO - [2023-03-18 02:45:30,472] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:30,557] {logging_mixin.py:115} INFO - [2023-03-18 02:45:30,552] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:45:30,558] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:30,580] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 02:46:00,670] {processor.py:153} INFO - Started process (PID=311) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:00,671] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:46:00,673] {logging_mixin.py:115} INFO - [2023-03-18 02:46:00,673] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:00,760] {logging_mixin.py:115} INFO - [2023-03-18 02:46:00,755] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:46:00,762] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:00,785] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.119 seconds
[2023-03-18 02:46:30,869] {processor.py:153} INFO - Started process (PID=316) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:30,870] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:46:30,872] {logging_mixin.py:115} INFO - [2023-03-18 02:46:30,872] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:30,950] {logging_mixin.py:115} INFO - [2023-03-18 02:46:30,945] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:46:30,952] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:30,971] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.106 seconds
[2023-03-18 02:47:01,059] {processor.py:153} INFO - Started process (PID=321) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:01,061] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:47:01,063] {logging_mixin.py:115} INFO - [2023-03-18 02:47:01,063] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:01,143] {logging_mixin.py:115} INFO - [2023-03-18 02:47:01,137] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:47:01,144] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:01,164] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:47:31,259] {processor.py:153} INFO - Started process (PID=326) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:31,260] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:47:31,262] {logging_mixin.py:115} INFO - [2023-03-18 02:47:31,262] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:31,345] {logging_mixin.py:115} INFO - [2023-03-18 02:47:31,340] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:47:31,346] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:31,368] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.113 seconds
[2023-03-18 02:48:01,458] {processor.py:153} INFO - Started process (PID=331) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:01,460] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:48:01,461] {logging_mixin.py:115} INFO - [2023-03-18 02:48:01,461] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:01,551] {logging_mixin.py:115} INFO - [2023-03-18 02:48:01,546] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:48:01,553] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:01,575] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.122 seconds
[2023-03-18 02:48:31,667] {processor.py:153} INFO - Started process (PID=336) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:31,668] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:48:31,670] {logging_mixin.py:115} INFO - [2023-03-18 02:48:31,670] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:31,753] {logging_mixin.py:115} INFO - [2023-03-18 02:48:31,749] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:48:31,755] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:31,777] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 02:49:01,863] {processor.py:153} INFO - Started process (PID=341) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:01,864] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:49:01,866] {logging_mixin.py:115} INFO - [2023-03-18 02:49:01,866] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:01,952] {logging_mixin.py:115} INFO - [2023-03-18 02:49:01,946] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:49:01,953] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:01,974] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 02:49:32,061] {processor.py:153} INFO - Started process (PID=346) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:32,063] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:49:32,066] {logging_mixin.py:115} INFO - [2023-03-18 02:49:32,066] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:32,151] {logging_mixin.py:115} INFO - [2023-03-18 02:49:32,146] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:49:32,152] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:32,175] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.119 seconds
[2023-03-18 02:50:02,269] {processor.py:153} INFO - Started process (PID=351) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:02,271] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:50:02,272] {logging_mixin.py:115} INFO - [2023-03-18 02:50:02,272] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:02,351] {logging_mixin.py:115} INFO - [2023-03-18 02:50:02,345] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:50:02,353] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:02,374] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.110 seconds
[2023-03-18 02:50:32,470] {processor.py:153} INFO - Started process (PID=356) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:32,472] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:50:32,473] {logging_mixin.py:115} INFO - [2023-03-18 02:50:32,473] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:32,554] {logging_mixin.py:115} INFO - [2023-03-18 02:50:32,549] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:50:32,556] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:32,581] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 02:51:02,673] {processor.py:153} INFO - Started process (PID=361) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:02,675] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:51:02,676] {logging_mixin.py:115} INFO - [2023-03-18 02:51:02,676] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:02,768] {logging_mixin.py:115} INFO - [2023-03-18 02:51:02,763] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:51:02,770] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:02,795] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.128 seconds
[2023-03-18 02:51:32,888] {processor.py:153} INFO - Started process (PID=366) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:32,889] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:51:32,891] {logging_mixin.py:115} INFO - [2023-03-18 02:51:32,891] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:32,969] {logging_mixin.py:115} INFO - [2023-03-18 02:51:32,964] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:51:32,971] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:32,990] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.107 seconds
[2023-03-18 02:52:03,084] {processor.py:153} INFO - Started process (PID=371) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:03,092] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:52:03,111] {logging_mixin.py:115} INFO - [2023-03-18 02:52:03,111] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:03,207] {logging_mixin.py:115} INFO - [2023-03-18 02:52:03,203] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:52:03,208] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:03,228] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.149 seconds
[2023-03-18 02:52:33,321] {processor.py:153} INFO - Started process (PID=376) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:33,324] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:52:33,325] {logging_mixin.py:115} INFO - [2023-03-18 02:52:33,325] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:33,408] {logging_mixin.py:115} INFO - [2023-03-18 02:52:33,403] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:52:33,409] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:33,439] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.124 seconds
[2023-03-18 02:53:03,534] {processor.py:153} INFO - Started process (PID=381) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:03,536] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:53:03,538] {logging_mixin.py:115} INFO - [2023-03-18 02:53:03,538] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:03,624] {logging_mixin.py:115} INFO - [2023-03-18 02:53:03,619] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:53:03,625] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:03,647] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.118 seconds
[2023-03-18 02:53:33,743] {processor.py:153} INFO - Started process (PID=386) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:33,747] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:53:33,750] {logging_mixin.py:115} INFO - [2023-03-18 02:53:33,750] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:33,841] {logging_mixin.py:115} INFO - [2023-03-18 02:53:33,836] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:53:33,844] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:33,866] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.127 seconds
[2023-03-18 02:54:03,959] {processor.py:153} INFO - Started process (PID=391) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:03,961] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:54:03,962] {logging_mixin.py:115} INFO - [2023-03-18 02:54:03,962] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:04,039] {logging_mixin.py:115} INFO - [2023-03-18 02:54:04,032] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:54:04,042] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:04,062] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.107 seconds
[2023-03-18 02:54:34,152] {processor.py:153} INFO - Started process (PID=396) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:34,154] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:54:34,155] {logging_mixin.py:115} INFO - [2023-03-18 02:54:34,155] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:34,235] {logging_mixin.py:115} INFO - [2023-03-18 02:54:34,229] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:54:34,237] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:34,257] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:55:04,347] {processor.py:153} INFO - Started process (PID=401) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:04,349] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:55:04,351] {logging_mixin.py:115} INFO - [2023-03-18 02:55:04,351] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:04,432] {logging_mixin.py:115} INFO - [2023-03-18 02:55:04,428] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:55:04,435] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:04,454] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.112 seconds
[2023-03-18 02:55:34,552] {processor.py:153} INFO - Started process (PID=406) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:34,553] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:55:34,555] {logging_mixin.py:115} INFO - [2023-03-18 02:55:34,555] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:34,633] {logging_mixin.py:115} INFO - [2023-03-18 02:55:34,628] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:55:34,637] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:34,656] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:56:04,749] {processor.py:153} INFO - Started process (PID=411) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:04,751] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:56:04,753] {logging_mixin.py:115} INFO - [2023-03-18 02:56:04,753] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:04,832] {logging_mixin.py:115} INFO - [2023-03-18 02:56:04,826] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:56:04,834] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:04,853] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:56:34,944] {processor.py:153} INFO - Started process (PID=416) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:34,946] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:56:34,948] {logging_mixin.py:115} INFO - [2023-03-18 02:56:34,948] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:35,047] {logging_mixin.py:115} INFO - [2023-03-18 02:56:35,041] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:56:35,052] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:35,075] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.136 seconds
[2023-03-18 02:57:05,173] {processor.py:153} INFO - Started process (PID=421) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:05,175] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:57:05,176] {logging_mixin.py:115} INFO - [2023-03-18 02:57:05,176] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:05,254] {logging_mixin.py:115} INFO - [2023-03-18 02:57:05,249] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:57:05,257] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:05,276] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.108 seconds
[2023-03-18 02:57:35,368] {processor.py:153} INFO - Started process (PID=426) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:35,369] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:57:35,371] {logging_mixin.py:115} INFO - [2023-03-18 02:57:35,371] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:35,470] {logging_mixin.py:115} INFO - [2023-03-18 02:57:35,465] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:57:35,474] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:35,501] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.138 seconds
[2023-03-18 02:58:05,595] {processor.py:153} INFO - Started process (PID=431) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:05,597] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:58:05,599] {logging_mixin.py:115} INFO - [2023-03-18 02:58:05,598] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:05,680] {logging_mixin.py:115} INFO - [2023-03-18 02:58:05,674] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:58:05,683] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:05,702] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.112 seconds
[2023-03-18 02:58:35,813] {processor.py:153} INFO - Started process (PID=436) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:35,816] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:58:35,818] {logging_mixin.py:115} INFO - [2023-03-18 02:58:35,818] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:35,948] {logging_mixin.py:115} INFO - [2023-03-18 02:58:35,943] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:58:35,950] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:35,992] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.186 seconds
[2023-03-18 02:59:06,087] {processor.py:153} INFO - Started process (PID=441) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:06,089] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:59:06,090] {logging_mixin.py:115} INFO - [2023-03-18 02:59:06,090] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:06,175] {logging_mixin.py:115} INFO - [2023-03-18 02:59:06,170] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:59:06,178] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:06,197] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 02:59:36,291] {processor.py:153} INFO - Started process (PID=446) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:36,292] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:59:36,294] {logging_mixin.py:115} INFO - [2023-03-18 02:59:36,294] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:36,374] {logging_mixin.py:115} INFO - [2023-03-18 02:59:36,368] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:59:36,376] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:36,395] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 03:00:06,488] {processor.py:153} INFO - Started process (PID=451) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:06,490] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:00:06,492] {logging_mixin.py:115} INFO - [2023-03-18 03:00:06,492] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:06,579] {logging_mixin.py:115} INFO - [2023-03-18 03:00:06,573] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:00:06,581] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:06,603] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.120 seconds
[2023-03-18 03:00:36,700] {processor.py:153} INFO - Started process (PID=456) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:36,701] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:00:36,704] {logging_mixin.py:115} INFO - [2023-03-18 03:00:36,703] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:36,817] {logging_mixin.py:115} INFO - [2023-03-18 03:00:36,809] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:00:36,822] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:36,855] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.160 seconds
[2023-03-18 03:01:06,952] {processor.py:153} INFO - Started process (PID=461) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:06,953] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:01:06,955] {logging_mixin.py:115} INFO - [2023-03-18 03:01:06,955] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:07,040] {logging_mixin.py:115} INFO - [2023-03-18 03:01:07,034] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:01:07,042] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:07,061] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 03:01:37,162] {processor.py:153} INFO - Started process (PID=466) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:37,164] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:01:37,166] {logging_mixin.py:115} INFO - [2023-03-18 03:01:37,166] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:37,300] {logging_mixin.py:115} INFO - [2023-03-18 03:01:37,294] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:01:37,303] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:37,328] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.174 seconds
[2023-03-18 03:02:07,421] {processor.py:153} INFO - Started process (PID=471) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:07,422] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:02:07,425] {logging_mixin.py:115} INFO - [2023-03-18 03:02:07,425] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:07,508] {logging_mixin.py:115} INFO - [2023-03-18 03:02:07,503] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:02:07,511] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:07,532] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 03:02:37,630] {processor.py:153} INFO - Started process (PID=476) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:37,631] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:02:37,633] {logging_mixin.py:115} INFO - [2023-03-18 03:02:37,633] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:37,714] {logging_mixin.py:115} INFO - [2023-03-18 03:02:37,707] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:02:37,717] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:37,737] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.111 seconds
[2023-03-18 03:03:07,825] {processor.py:153} INFO - Started process (PID=481) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:07,827] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:03:07,828] {logging_mixin.py:115} INFO - [2023-03-18 03:03:07,828] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:07,907] {logging_mixin.py:115} INFO - [2023-03-18 03:03:07,901] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:03:07,910] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:07,928] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.108 seconds
[2023-03-18 03:03:38,002] {processor.py:153} INFO - Started process (PID=486) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:38,004] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:03:38,007] {logging_mixin.py:115} INFO - [2023-03-18 03:03:38,006] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:38,183] {logging_mixin.py:115} INFO - [2023-03-18 03:03:38,146] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:03:38,187] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:38,226] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.228 seconds
[2023-03-18 03:04:08,332] {processor.py:153} INFO - Started process (PID=491) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:08,336] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:04:08,338] {logging_mixin.py:115} INFO - [2023-03-18 03:04:08,337] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:08,420] {logging_mixin.py:115} INFO - [2023-03-18 03:04:08,416] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:04:08,424] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:08,445] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.118 seconds
[2023-03-18 03:04:38,551] {processor.py:153} INFO - Started process (PID=496) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:38,555] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:04:38,557] {logging_mixin.py:115} INFO - [2023-03-18 03:04:38,557] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:38,661] {logging_mixin.py:115} INFO - [2023-03-18 03:04:38,656] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:04:38,664] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:38,689] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.147 seconds
[2023-03-18 03:05:08,782] {processor.py:153} INFO - Started process (PID=501) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:08,784] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:05:08,785] {logging_mixin.py:115} INFO - [2023-03-18 03:05:08,785] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:08,869] {logging_mixin.py:115} INFO - [2023-03-18 03:05:08,864] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:05:08,871] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:08,892] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 03:05:38,979] {processor.py:153} INFO - Started process (PID=506) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:38,981] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:05:38,982] {logging_mixin.py:115} INFO - [2023-03-18 03:05:38,982] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:39,066] {logging_mixin.py:115} INFO - [2023-03-18 03:05:39,061] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:05:39,069] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:39,091] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 03:06:09,177] {processor.py:153} INFO - Started process (PID=511) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:09,180] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:06:09,181] {logging_mixin.py:115} INFO - [2023-03-18 03:06:09,181] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:09,265] {logging_mixin.py:115} INFO - [2023-03-18 03:06:09,260] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:06:09,267] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:09,287] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 03:06:39,507] {processor.py:153} INFO - Started process (PID=516) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:39,516] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:06:39,522] {logging_mixin.py:115} INFO - [2023-03-18 03:06:39,522] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:39,975] {logging_mixin.py:115} INFO - [2023-03-18 03:06:39,890] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:06:40,081] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:40,342] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.845 seconds
[2023-03-18 03:07:12,146] {processor.py:153} INFO - Started process (PID=521) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:12,149] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:07:12,154] {logging_mixin.py:115} INFO - [2023-03-18 03:07:12,154] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:12,610] {logging_mixin.py:115} INFO - [2023-03-18 03:07:12,597] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:07:12,613] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:12,696] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.577 seconds
[2023-03-18 03:07:42,788] {processor.py:153} INFO - Started process (PID=526) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:42,789] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:07:42,791] {logging_mixin.py:115} INFO - [2023-03-18 03:07:42,791] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:42,923] {logging_mixin.py:115} INFO - [2023-03-18 03:07:42,917] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:07:42,926] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:42,951] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.167 seconds
[2023-03-18 03:08:13,038] {processor.py:153} INFO - Started process (PID=531) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:13,040] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:08:13,041] {logging_mixin.py:115} INFO - [2023-03-18 03:08:13,041] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:13,131] {logging_mixin.py:115} INFO - [2023-03-18 03:08:13,126] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:08:13,134] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:13,158] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.125 seconds
[2023-03-18 03:08:43,253] {processor.py:153} INFO - Started process (PID=536) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:43,254] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:08:43,256] {logging_mixin.py:115} INFO - [2023-03-18 03:08:43,256] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:43,338] {logging_mixin.py:115} INFO - [2023-03-18 03:08:43,334] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:08:43,340] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:43,361] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.112 seconds
[2023-03-18 03:09:13,451] {processor.py:153} INFO - Started process (PID=541) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:13,453] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:09:13,455] {logging_mixin.py:115} INFO - [2023-03-18 03:09:13,454] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:13,540] {logging_mixin.py:115} INFO - [2023-03-18 03:09:13,535] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:09:13,545] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:13,567] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.120 seconds
[2023-03-18 03:09:43,656] {processor.py:153} INFO - Started process (PID=546) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:43,658] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:09:43,659] {logging_mixin.py:115} INFO - [2023-03-18 03:09:43,659] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:43,743] {logging_mixin.py:115} INFO - [2023-03-18 03:09:43,738] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:09:43,745] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:43,767] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 03:10:13,860] {processor.py:153} INFO - Started process (PID=551) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:13,862] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:10:13,863] {logging_mixin.py:115} INFO - [2023-03-18 03:10:13,863] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:13,954] {logging_mixin.py:115} INFO - [2023-03-18 03:10:13,949] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:10:13,956] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:13,981] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.125 seconds
[2023-03-18 03:10:44,080] {processor.py:153} INFO - Started process (PID=556) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:44,082] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:10:44,085] {logging_mixin.py:115} INFO - [2023-03-18 03:10:44,085] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:44,196] {logging_mixin.py:115} INFO - [2023-03-18 03:10:44,190] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:10:44,199] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:44,224] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.150 seconds
[2023-03-18 03:11:14,306] {processor.py:153} INFO - Started process (PID=561) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:14,310] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:11:14,311] {logging_mixin.py:115} INFO - [2023-03-18 03:11:14,311] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:14,416] {logging_mixin.py:115} INFO - [2023-03-18 03:11:14,411] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:11:14,420] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:14,442] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.139 seconds
[2023-03-18 03:11:44,532] {processor.py:153} INFO - Started process (PID=566) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:44,533] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:11:44,535] {logging_mixin.py:115} INFO - [2023-03-18 03:11:44,535] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:44,629] {logging_mixin.py:115} INFO - [2023-03-18 03:11:44,624] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:11:44,631] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:44,652] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.123 seconds
[2023-03-18 03:12:14,996] {processor.py:153} INFO - Started process (PID=571) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:15,027] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:12:15,058] {logging_mixin.py:115} INFO - [2023-03-18 03:12:15,058] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:15,398] {logging_mixin.py:115} INFO - [2023-03-18 03:12:15,354] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:12:15,404] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:15,542] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.560 seconds
[2023-03-18 03:12:45,722] {processor.py:153} INFO - Started process (PID=576) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:45,724] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:12:45,726] {logging_mixin.py:115} INFO - [2023-03-18 03:12:45,726] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:45,812] {logging_mixin.py:115} INFO - [2023-03-18 03:12:45,808] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:12:45,815] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:45,838] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.121 seconds
[2023-03-18 03:13:16,057] {processor.py:153} INFO - Started process (PID=607) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:13:16,069] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:13:16,107] {logging_mixin.py:115} INFO - [2023-03-18 03:13:16,107] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:13:16,261] {logging_mixin.py:115} INFO - [2023-03-18 03:13:16,255] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:13:16,264] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:13:16,292] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.248 seconds
[2023-03-18 23:13:53,744] {processor.py:153} INFO - Started process (PID=179) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:13:53,746] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:13:53,747] {logging_mixin.py:115} INFO - [2023-03-18 23:13:53,747] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:13:54,145] {logging_mixin.py:115} INFO - [2023-03-18 23:13:54,138] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:13:54,149] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:13:54,209] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.469 seconds
[2023-03-18 23:14:24,951] {processor.py:153} INFO - Started process (PID=185) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:24,957] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:14:24,965] {logging_mixin.py:115} INFO - [2023-03-18 23:14:24,964] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:25,333] {logging_mixin.py:115} INFO - [2023-03-18 23:14:25,321] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:14:25,339] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:25,386] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.472 seconds
[2023-03-18 23:14:55,476] {processor.py:153} INFO - Started process (PID=190) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:55,478] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:14:55,480] {logging_mixin.py:115} INFO - [2023-03-18 23:14:55,480] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:55,574] {logging_mixin.py:115} INFO - [2023-03-18 23:14:55,561] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:14:55,577] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:55,606] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.134 seconds
[2023-03-18 23:15:25,704] {processor.py:153} INFO - Started process (PID=195) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:25,706] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:15:25,708] {logging_mixin.py:115} INFO - [2023-03-18 23:15:25,708] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:25,801] {logging_mixin.py:115} INFO - [2023-03-18 23:15:25,795] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:15:25,804] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:25,831] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.131 seconds
[2023-03-18 23:15:55,945] {processor.py:153} INFO - Started process (PID=200) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:55,947] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:15:55,949] {logging_mixin.py:115} INFO - [2023-03-18 23:15:55,949] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:56,032] {logging_mixin.py:115} INFO - [2023-03-18 23:15:56,026] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:15:56,034] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:56,056] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 23:16:26,152] {processor.py:153} INFO - Started process (PID=205) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:26,155] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:16:26,156] {logging_mixin.py:115} INFO - [2023-03-18 23:16:26,156] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:26,242] {logging_mixin.py:115} INFO - [2023-03-18 23:16:26,237] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:16:26,246] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:26,269] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.122 seconds
[2023-03-18 23:16:56,368] {processor.py:153} INFO - Started process (PID=210) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:56,369] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:16:56,371] {logging_mixin.py:115} INFO - [2023-03-18 23:16:56,370] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:56,462] {logging_mixin.py:115} INFO - [2023-03-18 23:16:56,457] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:16:56,465] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:56,660] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.297 seconds
[2023-03-18 23:17:26,760] {processor.py:153} INFO - Started process (PID=215) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:26,763] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:17:26,765] {logging_mixin.py:115} INFO - [2023-03-18 23:17:26,765] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:26,870] {logging_mixin.py:115} INFO - [2023-03-18 23:17:26,852] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:17:26,872] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:26,898] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.142 seconds
[2023-03-18 23:17:56,993] {processor.py:153} INFO - Started process (PID=220) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:56,996] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:17:56,997] {logging_mixin.py:115} INFO - [2023-03-18 23:17:56,997] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:57,080] {logging_mixin.py:115} INFO - [2023-03-18 23:17:57,075] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:17:57,082] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:57,104] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 23:18:27,278] {processor.py:153} INFO - Started process (PID=225) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:27,282] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:18:27,284] {logging_mixin.py:115} INFO - [2023-03-18 23:18:27,284] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:27,390] {logging_mixin.py:115} INFO - [2023-03-18 23:18:27,381] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:18:27,394] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:27,422] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.150 seconds
[2023-03-18 23:18:57,520] {processor.py:153} INFO - Started process (PID=230) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:57,523] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:18:57,525] {logging_mixin.py:115} INFO - [2023-03-18 23:18:57,524] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:57,609] {logging_mixin.py:115} INFO - [2023-03-18 23:18:57,603] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:18:57,612] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:57,637] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.123 seconds
[2023-03-18 23:19:27,731] {processor.py:153} INFO - Started process (PID=235) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:27,734] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:19:27,735] {logging_mixin.py:115} INFO - [2023-03-18 23:19:27,735] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:27,817] {logging_mixin.py:115} INFO - [2023-03-18 23:19:27,813] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:19:27,820] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:27,841] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 23:19:57,937] {processor.py:153} INFO - Started process (PID=240) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:57,938] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:19:57,940] {logging_mixin.py:115} INFO - [2023-03-18 23:19:57,940] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:58,019] {logging_mixin.py:115} INFO - [2023-03-18 23:19:58,012] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:19:58,024] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:58,045] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.113 seconds
[2023-03-18 23:20:28,180] {processor.py:153} INFO - Started process (PID=245) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:28,182] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:20:28,184] {logging_mixin.py:115} INFO - [2023-03-18 23:20:28,184] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:28,281] {logging_mixin.py:115} INFO - [2023-03-18 23:20:28,274] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:20:28,284] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:28,310] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.138 seconds
[2023-03-18 23:20:58,402] {processor.py:153} INFO - Started process (PID=250) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:58,404] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:20:58,405] {logging_mixin.py:115} INFO - [2023-03-18 23:20:58,405] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:58,495] {logging_mixin.py:115} INFO - [2023-03-18 23:20:58,489] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:20:58,497] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:58,524] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.127 seconds
[2023-03-18 23:21:28,618] {processor.py:153} INFO - Started process (PID=255) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:21:28,619] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:21:28,621] {logging_mixin.py:115} INFO - [2023-03-18 23:21:28,621] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:21:28,701] {logging_mixin.py:115} INFO - [2023-03-18 23:21:28,696] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:21:28,703] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:21:28,727] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
