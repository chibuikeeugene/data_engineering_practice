[2023-03-18 02:32:55,002] {processor.py:153} INFO - Started process (PID=180) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:32:55,004] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:32:55,005] {logging_mixin.py:115} INFO - [2023-03-18 02:32:55,005] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:32:55,161] {logging_mixin.py:115} INFO - [2023-03-18 02:32:55,156] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:32:55,162] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:32:55,185] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.186 seconds
[2023-03-18 02:33:25,300] {processor.py:153} INFO - Started process (PID=186) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:25,302] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:33:25,304] {logging_mixin.py:115} INFO - [2023-03-18 02:33:25,304] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:25,439] {logging_mixin.py:115} INFO - [2023-03-18 02:33:25,434] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:33:25,442] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:25,480] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.184 seconds
[2023-03-18 02:33:55,577] {processor.py:153} INFO - Started process (PID=191) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:55,578] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:33:55,579] {logging_mixin.py:115} INFO - [2023-03-18 02:33:55,579] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:55,670] {logging_mixin.py:115} INFO - [2023-03-18 02:33:55,665] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:33:55,671] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:33:55,693] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.121 seconds
[2023-03-18 02:34:25,790] {processor.py:153} INFO - Started process (PID=196) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:25,792] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:34:25,794] {logging_mixin.py:115} INFO - [2023-03-18 02:34:25,793] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:25,907] {logging_mixin.py:115} INFO - [2023-03-18 02:34:25,902] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:34:25,908] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:25,933] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.147 seconds
[2023-03-18 02:34:56,032] {processor.py:153} INFO - Started process (PID=201) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:56,034] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:34:56,035] {logging_mixin.py:115} INFO - [2023-03-18 02:34:56,035] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:56,118] {logging_mixin.py:115} INFO - [2023-03-18 02:34:56,114] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:34:56,120] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:34:56,140] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.113 seconds
[2023-03-18 02:35:26,231] {processor.py:153} INFO - Started process (PID=206) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:26,233] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:35:26,235] {logging_mixin.py:115} INFO - [2023-03-18 02:35:26,234] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:26,321] {logging_mixin.py:115} INFO - [2023-03-18 02:35:26,316] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:35:26,322] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:26,345] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.118 seconds
[2023-03-18 02:35:56,433] {processor.py:153} INFO - Started process (PID=211) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:56,435] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:35:56,436] {logging_mixin.py:115} INFO - [2023-03-18 02:35:56,436] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:56,524] {logging_mixin.py:115} INFO - [2023-03-18 02:35:56,518] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:35:56,525] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:35:56,546] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.117 seconds
[2023-03-18 02:36:26,635] {processor.py:153} INFO - Started process (PID=216) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:26,636] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:36:26,637] {logging_mixin.py:115} INFO - [2023-03-18 02:36:26,637] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:26,755] {logging_mixin.py:115} INFO - [2023-03-18 02:36:26,746] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:36:26,759] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:26,793] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.162 seconds
[2023-03-18 02:36:56,896] {processor.py:153} INFO - Started process (PID=221) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:56,898] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:36:56,900] {logging_mixin.py:115} INFO - [2023-03-18 02:36:56,900] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:56,989] {logging_mixin.py:115} INFO - [2023-03-18 02:36:56,984] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:36:56,990] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:36:57,014] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.122 seconds
[2023-03-18 02:37:27,105] {processor.py:153} INFO - Started process (PID=226) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:27,106] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:37:27,108] {logging_mixin.py:115} INFO - [2023-03-18 02:37:27,108] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:27,188] {logging_mixin.py:115} INFO - [2023-03-18 02:37:27,182] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:37:27,190] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:27,215] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 02:37:57,309] {processor.py:153} INFO - Started process (PID=231) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:57,311] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:37:57,312] {logging_mixin.py:115} INFO - [2023-03-18 02:37:57,312] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:57,392] {logging_mixin.py:115} INFO - [2023-03-18 02:37:57,379] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:37:57,394] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:37:57,415] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:38:27,510] {processor.py:153} INFO - Started process (PID=236) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:27,511] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:38:27,513] {logging_mixin.py:115} INFO - [2023-03-18 02:38:27,513] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:27,602] {logging_mixin.py:115} INFO - [2023-03-18 02:38:27,597] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:38:27,605] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:27,628] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.122 seconds
[2023-03-18 02:38:57,720] {processor.py:153} INFO - Started process (PID=241) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:57,722] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:38:57,723] {logging_mixin.py:115} INFO - [2023-03-18 02:38:57,723] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:57,809] {logging_mixin.py:115} INFO - [2023-03-18 02:38:57,804] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:38:57,811] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:38:57,833] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.117 seconds
[2023-03-18 02:39:27,920] {processor.py:153} INFO - Started process (PID=246) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:27,921] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:39:27,923] {logging_mixin.py:115} INFO - [2023-03-18 02:39:27,923] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:28,012] {logging_mixin.py:115} INFO - [2023-03-18 02:39:28,007] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:39:28,013] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:28,035] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.119 seconds
[2023-03-18 02:39:58,128] {processor.py:153} INFO - Started process (PID=251) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:58,130] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:39:58,131] {logging_mixin.py:115} INFO - [2023-03-18 02:39:58,131] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:58,211] {logging_mixin.py:115} INFO - [2023-03-18 02:39:58,205] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:39:58,213] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:39:58,233] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:40:28,322] {processor.py:153} INFO - Started process (PID=256) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:28,325] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:40:28,327] {logging_mixin.py:115} INFO - [2023-03-18 02:40:28,327] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:28,407] {logging_mixin.py:115} INFO - [2023-03-18 02:40:28,401] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:40:28,408] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:28,428] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.110 seconds
[2023-03-18 02:40:58,520] {processor.py:153} INFO - Started process (PID=261) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:58,522] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:40:58,523] {logging_mixin.py:115} INFO - [2023-03-18 02:40:58,523] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:58,596] {logging_mixin.py:115} INFO - [2023-03-18 02:40:58,592] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:40:58,597] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:40:58,618] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.103 seconds
[2023-03-18 02:41:28,717] {processor.py:153} INFO - Started process (PID=266) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:28,719] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:41:28,720] {logging_mixin.py:115} INFO - [2023-03-18 02:41:28,720] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:28,807] {logging_mixin.py:115} INFO - [2023-03-18 02:41:28,802] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:41:28,808] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:28,829] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.117 seconds
[2023-03-18 02:41:58,921] {processor.py:153} INFO - Started process (PID=271) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:58,923] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:41:58,925] {logging_mixin.py:115} INFO - [2023-03-18 02:41:58,925] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:59,007] {logging_mixin.py:115} INFO - [2023-03-18 02:41:59,003] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:41:59,009] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:41:59,033] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 02:42:29,128] {processor.py:153} INFO - Started process (PID=276) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:29,130] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:42:29,131] {logging_mixin.py:115} INFO - [2023-03-18 02:42:29,131] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:29,231] {logging_mixin.py:115} INFO - [2023-03-18 02:42:29,227] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:42:29,233] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:29,256] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.133 seconds
[2023-03-18 02:42:59,356] {processor.py:153} INFO - Started process (PID=281) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:59,357] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:42:59,359] {logging_mixin.py:115} INFO - [2023-03-18 02:42:59,358] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:59,439] {logging_mixin.py:115} INFO - [2023-03-18 02:42:59,433] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:42:59,440] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:42:59,460] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.108 seconds
[2023-03-18 02:43:29,557] {processor.py:153} INFO - Started process (PID=286) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:29,559] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:43:29,561] {logging_mixin.py:115} INFO - [2023-03-18 02:43:29,561] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:29,673] {logging_mixin.py:115} INFO - [2023-03-18 02:43:29,668] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:43:29,675] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:29,697] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.145 seconds
[2023-03-18 02:43:59,796] {processor.py:153} INFO - Started process (PID=291) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:59,798] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:43:59,799] {logging_mixin.py:115} INFO - [2023-03-18 02:43:59,799] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:59,919] {logging_mixin.py:115} INFO - [2023-03-18 02:43:59,913] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:43:59,921] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:43:59,951] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.163 seconds
[2023-03-18 02:44:30,053] {processor.py:153} INFO - Started process (PID=296) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:44:30,055] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:44:30,056] {logging_mixin.py:115} INFO - [2023-03-18 02:44:30,056] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:44:30,141] {logging_mixin.py:115} INFO - [2023-03-18 02:44:30,136] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:44:30,142] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:44:30,163] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 02:45:00,259] {processor.py:153} INFO - Started process (PID=301) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:00,260] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:45:00,262] {logging_mixin.py:115} INFO - [2023-03-18 02:45:00,262] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:00,349] {logging_mixin.py:115} INFO - [2023-03-18 02:45:00,344] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:45:00,351] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:00,374] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.120 seconds
[2023-03-18 02:45:30,469] {processor.py:153} INFO - Started process (PID=306) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:30,471] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:45:30,472] {logging_mixin.py:115} INFO - [2023-03-18 02:45:30,472] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:30,557] {logging_mixin.py:115} INFO - [2023-03-18 02:45:30,552] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:45:30,558] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:45:30,580] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 02:46:00,670] {processor.py:153} INFO - Started process (PID=311) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:00,671] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:46:00,673] {logging_mixin.py:115} INFO - [2023-03-18 02:46:00,673] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:00,760] {logging_mixin.py:115} INFO - [2023-03-18 02:46:00,755] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:46:00,762] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:00,785] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.119 seconds
[2023-03-18 02:46:30,869] {processor.py:153} INFO - Started process (PID=316) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:30,870] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:46:30,872] {logging_mixin.py:115} INFO - [2023-03-18 02:46:30,872] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:30,950] {logging_mixin.py:115} INFO - [2023-03-18 02:46:30,945] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:46:30,952] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:46:30,971] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.106 seconds
[2023-03-18 02:47:01,059] {processor.py:153} INFO - Started process (PID=321) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:01,061] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:47:01,063] {logging_mixin.py:115} INFO - [2023-03-18 02:47:01,063] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:01,143] {logging_mixin.py:115} INFO - [2023-03-18 02:47:01,137] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:47:01,144] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:01,164] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:47:31,259] {processor.py:153} INFO - Started process (PID=326) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:31,260] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:47:31,262] {logging_mixin.py:115} INFO - [2023-03-18 02:47:31,262] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:31,345] {logging_mixin.py:115} INFO - [2023-03-18 02:47:31,340] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:47:31,346] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:47:31,368] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.113 seconds
[2023-03-18 02:48:01,458] {processor.py:153} INFO - Started process (PID=331) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:01,460] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:48:01,461] {logging_mixin.py:115} INFO - [2023-03-18 02:48:01,461] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:01,551] {logging_mixin.py:115} INFO - [2023-03-18 02:48:01,546] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:48:01,553] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:01,575] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.122 seconds
[2023-03-18 02:48:31,667] {processor.py:153} INFO - Started process (PID=336) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:31,668] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:48:31,670] {logging_mixin.py:115} INFO - [2023-03-18 02:48:31,670] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:31,753] {logging_mixin.py:115} INFO - [2023-03-18 02:48:31,749] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:48:31,755] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:48:31,777] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 02:49:01,863] {processor.py:153} INFO - Started process (PID=341) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:01,864] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:49:01,866] {logging_mixin.py:115} INFO - [2023-03-18 02:49:01,866] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:01,952] {logging_mixin.py:115} INFO - [2023-03-18 02:49:01,946] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:49:01,953] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:01,974] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 02:49:32,061] {processor.py:153} INFO - Started process (PID=346) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:32,063] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:49:32,066] {logging_mixin.py:115} INFO - [2023-03-18 02:49:32,066] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:32,151] {logging_mixin.py:115} INFO - [2023-03-18 02:49:32,146] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:49:32,152] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:49:32,175] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.119 seconds
[2023-03-18 02:50:02,269] {processor.py:153} INFO - Started process (PID=351) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:02,271] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:50:02,272] {logging_mixin.py:115} INFO - [2023-03-18 02:50:02,272] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:02,351] {logging_mixin.py:115} INFO - [2023-03-18 02:50:02,345] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:50:02,353] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:02,374] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.110 seconds
[2023-03-18 02:50:32,470] {processor.py:153} INFO - Started process (PID=356) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:32,472] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:50:32,473] {logging_mixin.py:115} INFO - [2023-03-18 02:50:32,473] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:32,554] {logging_mixin.py:115} INFO - [2023-03-18 02:50:32,549] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:50:32,556] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:50:32,581] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 02:51:02,673] {processor.py:153} INFO - Started process (PID=361) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:02,675] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:51:02,676] {logging_mixin.py:115} INFO - [2023-03-18 02:51:02,676] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:02,768] {logging_mixin.py:115} INFO - [2023-03-18 02:51:02,763] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:51:02,770] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:02,795] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.128 seconds
[2023-03-18 02:51:32,888] {processor.py:153} INFO - Started process (PID=366) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:32,889] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:51:32,891] {logging_mixin.py:115} INFO - [2023-03-18 02:51:32,891] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:32,969] {logging_mixin.py:115} INFO - [2023-03-18 02:51:32,964] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:51:32,971] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:51:32,990] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.107 seconds
[2023-03-18 02:52:03,084] {processor.py:153} INFO - Started process (PID=371) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:03,092] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:52:03,111] {logging_mixin.py:115} INFO - [2023-03-18 02:52:03,111] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:03,207] {logging_mixin.py:115} INFO - [2023-03-18 02:52:03,203] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:52:03,208] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:03,228] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.149 seconds
[2023-03-18 02:52:33,321] {processor.py:153} INFO - Started process (PID=376) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:33,324] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:52:33,325] {logging_mixin.py:115} INFO - [2023-03-18 02:52:33,325] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:33,408] {logging_mixin.py:115} INFO - [2023-03-18 02:52:33,403] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:52:33,409] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:52:33,439] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.124 seconds
[2023-03-18 02:53:03,534] {processor.py:153} INFO - Started process (PID=381) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:03,536] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:53:03,538] {logging_mixin.py:115} INFO - [2023-03-18 02:53:03,538] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:03,624] {logging_mixin.py:115} INFO - [2023-03-18 02:53:03,619] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:53:03,625] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:03,647] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.118 seconds
[2023-03-18 02:53:33,743] {processor.py:153} INFO - Started process (PID=386) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:33,747] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:53:33,750] {logging_mixin.py:115} INFO - [2023-03-18 02:53:33,750] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:33,841] {logging_mixin.py:115} INFO - [2023-03-18 02:53:33,836] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:53:33,844] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:53:33,866] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.127 seconds
[2023-03-18 02:54:03,959] {processor.py:153} INFO - Started process (PID=391) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:03,961] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:54:03,962] {logging_mixin.py:115} INFO - [2023-03-18 02:54:03,962] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:04,039] {logging_mixin.py:115} INFO - [2023-03-18 02:54:04,032] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:54:04,042] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:04,062] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.107 seconds
[2023-03-18 02:54:34,152] {processor.py:153} INFO - Started process (PID=396) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:34,154] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:54:34,155] {logging_mixin.py:115} INFO - [2023-03-18 02:54:34,155] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:34,235] {logging_mixin.py:115} INFO - [2023-03-18 02:54:34,229] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:54:34,237] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:54:34,257] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:55:04,347] {processor.py:153} INFO - Started process (PID=401) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:04,349] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:55:04,351] {logging_mixin.py:115} INFO - [2023-03-18 02:55:04,351] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:04,432] {logging_mixin.py:115} INFO - [2023-03-18 02:55:04,428] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:55:04,435] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:04,454] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.112 seconds
[2023-03-18 02:55:34,552] {processor.py:153} INFO - Started process (PID=406) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:34,553] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:55:34,555] {logging_mixin.py:115} INFO - [2023-03-18 02:55:34,555] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:34,633] {logging_mixin.py:115} INFO - [2023-03-18 02:55:34,628] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:55:34,637] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:55:34,656] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:56:04,749] {processor.py:153} INFO - Started process (PID=411) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:04,751] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:56:04,753] {logging_mixin.py:115} INFO - [2023-03-18 02:56:04,753] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:04,832] {logging_mixin.py:115} INFO - [2023-03-18 02:56:04,826] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:56:04,834] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:04,853] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 02:56:34,944] {processor.py:153} INFO - Started process (PID=416) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:34,946] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:56:34,948] {logging_mixin.py:115} INFO - [2023-03-18 02:56:34,948] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:35,047] {logging_mixin.py:115} INFO - [2023-03-18 02:56:35,041] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:56:35,052] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:56:35,075] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.136 seconds
[2023-03-18 02:57:05,173] {processor.py:153} INFO - Started process (PID=421) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:05,175] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:57:05,176] {logging_mixin.py:115} INFO - [2023-03-18 02:57:05,176] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:05,254] {logging_mixin.py:115} INFO - [2023-03-18 02:57:05,249] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:57:05,257] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:05,276] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.108 seconds
[2023-03-18 02:57:35,368] {processor.py:153} INFO - Started process (PID=426) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:35,369] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:57:35,371] {logging_mixin.py:115} INFO - [2023-03-18 02:57:35,371] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:35,470] {logging_mixin.py:115} INFO - [2023-03-18 02:57:35,465] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:57:35,474] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:57:35,501] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.138 seconds
[2023-03-18 02:58:05,595] {processor.py:153} INFO - Started process (PID=431) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:05,597] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:58:05,599] {logging_mixin.py:115} INFO - [2023-03-18 02:58:05,598] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:05,680] {logging_mixin.py:115} INFO - [2023-03-18 02:58:05,674] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:58:05,683] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:05,702] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.112 seconds
[2023-03-18 02:58:35,813] {processor.py:153} INFO - Started process (PID=436) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:35,816] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:58:35,818] {logging_mixin.py:115} INFO - [2023-03-18 02:58:35,818] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:35,948] {logging_mixin.py:115} INFO - [2023-03-18 02:58:35,943] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:58:35,950] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:58:35,992] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.186 seconds
[2023-03-18 02:59:06,087] {processor.py:153} INFO - Started process (PID=441) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:06,089] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:59:06,090] {logging_mixin.py:115} INFO - [2023-03-18 02:59:06,090] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:06,175] {logging_mixin.py:115} INFO - [2023-03-18 02:59:06,170] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:59:06,178] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:06,197] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 02:59:36,291] {processor.py:153} INFO - Started process (PID=446) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:36,292] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 02:59:36,294] {logging_mixin.py:115} INFO - [2023-03-18 02:59:36,294] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:36,374] {logging_mixin.py:115} INFO - [2023-03-18 02:59:36,368] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 02:59:36,376] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 02:59:36,395] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.109 seconds
[2023-03-18 03:00:06,488] {processor.py:153} INFO - Started process (PID=451) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:06,490] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:00:06,492] {logging_mixin.py:115} INFO - [2023-03-18 03:00:06,492] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:06,579] {logging_mixin.py:115} INFO - [2023-03-18 03:00:06,573] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:00:06,581] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:06,603] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.120 seconds
[2023-03-18 03:00:36,700] {processor.py:153} INFO - Started process (PID=456) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:36,701] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:00:36,704] {logging_mixin.py:115} INFO - [2023-03-18 03:00:36,703] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:36,817] {logging_mixin.py:115} INFO - [2023-03-18 03:00:36,809] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:00:36,822] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:00:36,855] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.160 seconds
[2023-03-18 03:01:06,952] {processor.py:153} INFO - Started process (PID=461) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:06,953] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:01:06,955] {logging_mixin.py:115} INFO - [2023-03-18 03:01:06,955] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:07,040] {logging_mixin.py:115} INFO - [2023-03-18 03:01:07,034] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:01:07,042] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:07,061] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 03:01:37,162] {processor.py:153} INFO - Started process (PID=466) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:37,164] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:01:37,166] {logging_mixin.py:115} INFO - [2023-03-18 03:01:37,166] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:37,300] {logging_mixin.py:115} INFO - [2023-03-18 03:01:37,294] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:01:37,303] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:01:37,328] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.174 seconds
[2023-03-18 03:02:07,421] {processor.py:153} INFO - Started process (PID=471) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:07,422] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:02:07,425] {logging_mixin.py:115} INFO - [2023-03-18 03:02:07,425] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:07,508] {logging_mixin.py:115} INFO - [2023-03-18 03:02:07,503] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:02:07,511] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:07,532] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 03:02:37,630] {processor.py:153} INFO - Started process (PID=476) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:37,631] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:02:37,633] {logging_mixin.py:115} INFO - [2023-03-18 03:02:37,633] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:37,714] {logging_mixin.py:115} INFO - [2023-03-18 03:02:37,707] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:02:37,717] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:02:37,737] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.111 seconds
[2023-03-18 03:03:07,825] {processor.py:153} INFO - Started process (PID=481) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:07,827] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:03:07,828] {logging_mixin.py:115} INFO - [2023-03-18 03:03:07,828] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:07,907] {logging_mixin.py:115} INFO - [2023-03-18 03:03:07,901] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:03:07,910] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:07,928] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.108 seconds
[2023-03-18 03:03:38,002] {processor.py:153} INFO - Started process (PID=486) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:38,004] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:03:38,007] {logging_mixin.py:115} INFO - [2023-03-18 03:03:38,006] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:38,183] {logging_mixin.py:115} INFO - [2023-03-18 03:03:38,146] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:03:38,187] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:03:38,226] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.228 seconds
[2023-03-18 03:04:08,332] {processor.py:153} INFO - Started process (PID=491) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:08,336] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:04:08,338] {logging_mixin.py:115} INFO - [2023-03-18 03:04:08,337] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:08,420] {logging_mixin.py:115} INFO - [2023-03-18 03:04:08,416] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:04:08,424] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:08,445] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.118 seconds
[2023-03-18 03:04:38,551] {processor.py:153} INFO - Started process (PID=496) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:38,555] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:04:38,557] {logging_mixin.py:115} INFO - [2023-03-18 03:04:38,557] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:38,661] {logging_mixin.py:115} INFO - [2023-03-18 03:04:38,656] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:04:38,664] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:04:38,689] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.147 seconds
[2023-03-18 03:05:08,782] {processor.py:153} INFO - Started process (PID=501) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:08,784] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:05:08,785] {logging_mixin.py:115} INFO - [2023-03-18 03:05:08,785] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:08,869] {logging_mixin.py:115} INFO - [2023-03-18 03:05:08,864] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:05:08,871] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:08,892] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 03:05:38,979] {processor.py:153} INFO - Started process (PID=506) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:38,981] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:05:38,982] {logging_mixin.py:115} INFO - [2023-03-18 03:05:38,982] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:39,066] {logging_mixin.py:115} INFO - [2023-03-18 03:05:39,061] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:05:39,069] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:05:39,091] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 03:06:09,177] {processor.py:153} INFO - Started process (PID=511) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:09,180] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:06:09,181] {logging_mixin.py:115} INFO - [2023-03-18 03:06:09,181] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:09,265] {logging_mixin.py:115} INFO - [2023-03-18 03:06:09,260] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:06:09,267] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:09,287] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 03:06:39,507] {processor.py:153} INFO - Started process (PID=516) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:39,516] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:06:39,522] {logging_mixin.py:115} INFO - [2023-03-18 03:06:39,522] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:39,975] {logging_mixin.py:115} INFO - [2023-03-18 03:06:39,890] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:06:40,081] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:06:40,342] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.845 seconds
[2023-03-18 03:07:12,146] {processor.py:153} INFO - Started process (PID=521) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:12,149] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:07:12,154] {logging_mixin.py:115} INFO - [2023-03-18 03:07:12,154] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:12,610] {logging_mixin.py:115} INFO - [2023-03-18 03:07:12,597] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:07:12,613] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:12,696] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.577 seconds
[2023-03-18 03:07:42,788] {processor.py:153} INFO - Started process (PID=526) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:42,789] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:07:42,791] {logging_mixin.py:115} INFO - [2023-03-18 03:07:42,791] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:42,923] {logging_mixin.py:115} INFO - [2023-03-18 03:07:42,917] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:07:42,926] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:07:42,951] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.167 seconds
[2023-03-18 03:08:13,038] {processor.py:153} INFO - Started process (PID=531) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:13,040] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:08:13,041] {logging_mixin.py:115} INFO - [2023-03-18 03:08:13,041] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:13,131] {logging_mixin.py:115} INFO - [2023-03-18 03:08:13,126] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:08:13,134] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:13,158] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.125 seconds
[2023-03-18 03:08:43,253] {processor.py:153} INFO - Started process (PID=536) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:43,254] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:08:43,256] {logging_mixin.py:115} INFO - [2023-03-18 03:08:43,256] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:43,338] {logging_mixin.py:115} INFO - [2023-03-18 03:08:43,334] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:08:43,340] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:08:43,361] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.112 seconds
[2023-03-18 03:09:13,451] {processor.py:153} INFO - Started process (PID=541) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:13,453] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:09:13,455] {logging_mixin.py:115} INFO - [2023-03-18 03:09:13,454] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:13,540] {logging_mixin.py:115} INFO - [2023-03-18 03:09:13,535] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:09:13,545] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:13,567] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.120 seconds
[2023-03-18 03:09:43,656] {processor.py:153} INFO - Started process (PID=546) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:43,658] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:09:43,659] {logging_mixin.py:115} INFO - [2023-03-18 03:09:43,659] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:43,743] {logging_mixin.py:115} INFO - [2023-03-18 03:09:43,738] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:09:43,745] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:09:43,767] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 03:10:13,860] {processor.py:153} INFO - Started process (PID=551) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:13,862] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:10:13,863] {logging_mixin.py:115} INFO - [2023-03-18 03:10:13,863] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:13,954] {logging_mixin.py:115} INFO - [2023-03-18 03:10:13,949] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:10:13,956] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:13,981] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.125 seconds
[2023-03-18 03:10:44,080] {processor.py:153} INFO - Started process (PID=556) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:44,082] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:10:44,085] {logging_mixin.py:115} INFO - [2023-03-18 03:10:44,085] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:44,196] {logging_mixin.py:115} INFO - [2023-03-18 03:10:44,190] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:10:44,199] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:10:44,224] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.150 seconds
[2023-03-18 03:11:14,306] {processor.py:153} INFO - Started process (PID=561) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:14,310] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:11:14,311] {logging_mixin.py:115} INFO - [2023-03-18 03:11:14,311] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:14,416] {logging_mixin.py:115} INFO - [2023-03-18 03:11:14,411] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:11:14,420] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:14,442] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.139 seconds
[2023-03-18 03:11:44,532] {processor.py:153} INFO - Started process (PID=566) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:44,533] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:11:44,535] {logging_mixin.py:115} INFO - [2023-03-18 03:11:44,535] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:44,629] {logging_mixin.py:115} INFO - [2023-03-18 03:11:44,624] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:11:44,631] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:11:44,652] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.123 seconds
[2023-03-18 03:12:14,996] {processor.py:153} INFO - Started process (PID=571) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:15,027] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:12:15,058] {logging_mixin.py:115} INFO - [2023-03-18 03:12:15,058] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:15,398] {logging_mixin.py:115} INFO - [2023-03-18 03:12:15,354] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:12:15,404] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:15,542] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.560 seconds
[2023-03-18 03:12:45,722] {processor.py:153} INFO - Started process (PID=576) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:45,724] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:12:45,726] {logging_mixin.py:115} INFO - [2023-03-18 03:12:45,726] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:45,812] {logging_mixin.py:115} INFO - [2023-03-18 03:12:45,808] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:12:45,815] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:12:45,838] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.121 seconds
[2023-03-18 03:13:16,057] {processor.py:153} INFO - Started process (PID=607) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:13:16,069] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 03:13:16,107] {logging_mixin.py:115} INFO - [2023-03-18 03:13:16,107] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:13:16,261] {logging_mixin.py:115} INFO - [2023-03-18 03:13:16,255] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 03:13:16,264] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 03:13:16,292] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.248 seconds
[2023-03-18 23:13:53,744] {processor.py:153} INFO - Started process (PID=179) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:13:53,746] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:13:53,747] {logging_mixin.py:115} INFO - [2023-03-18 23:13:53,747] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:13:54,145] {logging_mixin.py:115} INFO - [2023-03-18 23:13:54,138] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:13:54,149] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:13:54,209] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.469 seconds
[2023-03-18 23:14:24,951] {processor.py:153} INFO - Started process (PID=185) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:24,957] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:14:24,965] {logging_mixin.py:115} INFO - [2023-03-18 23:14:24,964] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:25,333] {logging_mixin.py:115} INFO - [2023-03-18 23:14:25,321] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:14:25,339] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:25,386] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.472 seconds
[2023-03-18 23:14:55,476] {processor.py:153} INFO - Started process (PID=190) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:55,478] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:14:55,480] {logging_mixin.py:115} INFO - [2023-03-18 23:14:55,480] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:55,574] {logging_mixin.py:115} INFO - [2023-03-18 23:14:55,561] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:14:55,577] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:14:55,606] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.134 seconds
[2023-03-18 23:15:25,704] {processor.py:153} INFO - Started process (PID=195) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:25,706] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:15:25,708] {logging_mixin.py:115} INFO - [2023-03-18 23:15:25,708] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:25,801] {logging_mixin.py:115} INFO - [2023-03-18 23:15:25,795] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:15:25,804] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:25,831] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.131 seconds
[2023-03-18 23:15:55,945] {processor.py:153} INFO - Started process (PID=200) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:55,947] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:15:55,949] {logging_mixin.py:115} INFO - [2023-03-18 23:15:55,949] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:56,032] {logging_mixin.py:115} INFO - [2023-03-18 23:15:56,026] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:15:56,034] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:15:56,056] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.116 seconds
[2023-03-18 23:16:26,152] {processor.py:153} INFO - Started process (PID=205) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:26,155] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:16:26,156] {logging_mixin.py:115} INFO - [2023-03-18 23:16:26,156] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:26,242] {logging_mixin.py:115} INFO - [2023-03-18 23:16:26,237] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:16:26,246] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:26,269] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.122 seconds
[2023-03-18 23:16:56,368] {processor.py:153} INFO - Started process (PID=210) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:56,369] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:16:56,371] {logging_mixin.py:115} INFO - [2023-03-18 23:16:56,370] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:56,462] {logging_mixin.py:115} INFO - [2023-03-18 23:16:56,457] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:16:56,465] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:16:56,660] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.297 seconds
[2023-03-18 23:17:26,760] {processor.py:153} INFO - Started process (PID=215) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:26,763] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:17:26,765] {logging_mixin.py:115} INFO - [2023-03-18 23:17:26,765] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:26,870] {logging_mixin.py:115} INFO - [2023-03-18 23:17:26,852] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:17:26,872] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:26,898] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.142 seconds
[2023-03-18 23:17:56,993] {processor.py:153} INFO - Started process (PID=220) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:56,996] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:17:56,997] {logging_mixin.py:115} INFO - [2023-03-18 23:17:56,997] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:57,080] {logging_mixin.py:115} INFO - [2023-03-18 23:17:57,075] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:17:57,082] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:17:57,104] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 23:18:27,278] {processor.py:153} INFO - Started process (PID=225) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:27,282] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:18:27,284] {logging_mixin.py:115} INFO - [2023-03-18 23:18:27,284] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:27,390] {logging_mixin.py:115} INFO - [2023-03-18 23:18:27,381] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:18:27,394] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:27,422] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.150 seconds
[2023-03-18 23:18:57,520] {processor.py:153} INFO - Started process (PID=230) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:57,523] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:18:57,525] {logging_mixin.py:115} INFO - [2023-03-18 23:18:57,524] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:57,609] {logging_mixin.py:115} INFO - [2023-03-18 23:18:57,603] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:18:57,612] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:18:57,637] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.123 seconds
[2023-03-18 23:19:27,731] {processor.py:153} INFO - Started process (PID=235) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:27,734] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:19:27,735] {logging_mixin.py:115} INFO - [2023-03-18 23:19:27,735] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:27,817] {logging_mixin.py:115} INFO - [2023-03-18 23:19:27,813] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:19:27,820] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:27,841] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 23:19:57,937] {processor.py:153} INFO - Started process (PID=240) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:57,938] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:19:57,940] {logging_mixin.py:115} INFO - [2023-03-18 23:19:57,940] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:58,019] {logging_mixin.py:115} INFO - [2023-03-18 23:19:58,012] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:19:58,024] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:19:58,045] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.113 seconds
[2023-03-18 23:20:28,180] {processor.py:153} INFO - Started process (PID=245) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:28,182] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:20:28,184] {logging_mixin.py:115} INFO - [2023-03-18 23:20:28,184] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:28,281] {logging_mixin.py:115} INFO - [2023-03-18 23:20:28,274] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:20:28,284] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:28,310] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.138 seconds
[2023-03-18 23:20:58,402] {processor.py:153} INFO - Started process (PID=250) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:58,404] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:20:58,405] {logging_mixin.py:115} INFO - [2023-03-18 23:20:58,405] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:58,495] {logging_mixin.py:115} INFO - [2023-03-18 23:20:58,489] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:20:58,497] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:20:58,524] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.127 seconds
[2023-03-18 23:21:28,618] {processor.py:153} INFO - Started process (PID=255) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:21:28,619] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:21:28,621] {logging_mixin.py:115} INFO - [2023-03-18 23:21:28,621] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:21:28,701] {logging_mixin.py:115} INFO - [2023-03-18 23:21:28,696] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:21:28,703] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:21:28,727] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 23:21:58,821] {processor.py:153} INFO - Started process (PID=260) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:21:58,823] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:21:58,825] {logging_mixin.py:115} INFO - [2023-03-18 23:21:58,824] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:21:58,908] {logging_mixin.py:115} INFO - [2023-03-18 23:21:58,903] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:21:58,911] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:21:58,933] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.117 seconds
[2023-03-18 23:22:29,028] {processor.py:153} INFO - Started process (PID=265) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:22:29,030] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:22:29,031] {logging_mixin.py:115} INFO - [2023-03-18 23:22:29,031] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:22:29,119] {logging_mixin.py:115} INFO - [2023-03-18 23:22:29,112] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:22:29,121] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:22:29,144] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.121 seconds
[2023-03-18 23:22:59,236] {processor.py:153} INFO - Started process (PID=270) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:22:59,238] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:22:59,239] {logging_mixin.py:115} INFO - [2023-03-18 23:22:59,239] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:22:59,333] {logging_mixin.py:115} INFO - [2023-03-18 23:22:59,329] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:22:59,336] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:22:59,360] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.129 seconds
[2023-03-18 23:23:29,449] {processor.py:153} INFO - Started process (PID=275) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:23:29,450] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:23:29,452] {logging_mixin.py:115} INFO - [2023-03-18 23:23:29,452] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:23:29,536] {logging_mixin.py:115} INFO - [2023-03-18 23:23:29,530] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:23:29,539] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:23:29,563] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.120 seconds
[2023-03-18 23:23:59,649] {processor.py:153} INFO - Started process (PID=280) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:23:59,651] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:23:59,652] {logging_mixin.py:115} INFO - [2023-03-18 23:23:59,652] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:23:59,740] {logging_mixin.py:115} INFO - [2023-03-18 23:23:59,734] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:23:59,743] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:23:59,769] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.124 seconds
[2023-03-18 23:24:29,831] {processor.py:153} INFO - Started process (PID=285) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:24:29,833] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:24:29,835] {logging_mixin.py:115} INFO - [2023-03-18 23:24:29,834] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:24:29,917] {logging_mixin.py:115} INFO - [2023-03-18 23:24:29,912] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:24:29,919] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:24:29,940] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 23:27:51,170] {processor.py:153} INFO - Started process (PID=290) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:27:51,173] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:27:51,176] {logging_mixin.py:115} INFO - [2023-03-18 23:27:51,176] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:27:51,310] {logging_mixin.py:115} INFO - [2023-03-18 23:27:51,305] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:27:51,313] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:27:51,338] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.179 seconds
[2023-03-18 23:28:21,452] {processor.py:153} INFO - Started process (PID=295) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:28:21,454] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:28:21,456] {logging_mixin.py:115} INFO - [2023-03-18 23:28:21,455] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:28:21,558] {logging_mixin.py:115} INFO - [2023-03-18 23:28:21,554] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:28:21,561] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:28:21,584] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.138 seconds
[2023-03-18 23:28:51,683] {processor.py:153} INFO - Started process (PID=300) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:28:51,685] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:28:51,686] {logging_mixin.py:115} INFO - [2023-03-18 23:28:51,686] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:28:51,774] {logging_mixin.py:115} INFO - [2023-03-18 23:28:51,768] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:28:51,776] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:28:51,799] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.120 seconds
[2023-03-18 23:31:31,583] {processor.py:153} INFO - Started process (PID=180) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:31:31,585] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:31:31,587] {logging_mixin.py:115} INFO - [2023-03-18 23:31:31,587] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:31:31,793] {logging_mixin.py:115} INFO - [2023-03-18 23:31:31,786] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:31:31,810] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:31:31,838] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.261 seconds
[2023-03-18 23:32:02,831] {processor.py:153} INFO - Started process (PID=186) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:32:02,839] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:32:02,845] {logging_mixin.py:115} INFO - [2023-03-18 23:32:02,845] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:32:03,427] {logging_mixin.py:115} INFO - [2023-03-18 23:32:03,408] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:32:03,430] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:32:03,485] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.717 seconds
[2023-03-18 23:32:33,586] {processor.py:153} INFO - Started process (PID=191) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:32:33,587] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:32:33,590] {logging_mixin.py:115} INFO - [2023-03-18 23:32:33,590] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:32:33,673] {logging_mixin.py:115} INFO - [2023-03-18 23:32:33,669] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:32:33,677] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:32:33,696] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 23:33:03,784] {processor.py:153} INFO - Started process (PID=196) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:33:03,786] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:33:03,788] {logging_mixin.py:115} INFO - [2023-03-18 23:33:03,787] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:33:03,870] {logging_mixin.py:115} INFO - [2023-03-18 23:33:03,865] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:33:03,873] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:33:03,893] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.112 seconds
[2023-03-18 23:33:33,983] {processor.py:153} INFO - Started process (PID=201) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:33:33,985] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:33:33,986] {logging_mixin.py:115} INFO - [2023-03-18 23:33:33,986] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:33:34,079] {logging_mixin.py:115} INFO - [2023-03-18 23:33:34,073] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:33:34,081] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:33:34,103] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.124 seconds
[2023-03-18 23:34:04,197] {processor.py:153} INFO - Started process (PID=206) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:34:04,198] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:34:04,201] {logging_mixin.py:115} INFO - [2023-03-18 23:34:04,201] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:34:04,316] {logging_mixin.py:115} INFO - [2023-03-18 23:34:04,308] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:34:04,320] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:34:04,349] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.158 seconds
[2023-03-18 23:34:34,450] {processor.py:153} INFO - Started process (PID=211) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:34:34,453] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:34:34,455] {logging_mixin.py:115} INFO - [2023-03-18 23:34:34,455] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:34:34,537] {logging_mixin.py:115} INFO - [2023-03-18 23:34:34,533] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:34:34,541] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:34:34,563] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.118 seconds
[2023-03-18 23:35:04,655] {processor.py:153} INFO - Started process (PID=216) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:35:04,656] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:35:04,658] {logging_mixin.py:115} INFO - [2023-03-18 23:35:04,658] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:35:04,742] {logging_mixin.py:115} INFO - [2023-03-18 23:35:04,737] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:35:04,744] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:35:04,767] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.117 seconds
[2023-03-18 23:35:34,854] {processor.py:153} INFO - Started process (PID=221) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:35:34,856] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:35:34,857] {logging_mixin.py:115} INFO - [2023-03-18 23:35:34,857] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:35:34,940] {logging_mixin.py:115} INFO - [2023-03-18 23:35:34,935] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:35:34,942] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:35:34,964] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.114 seconds
[2023-03-18 23:36:05,065] {processor.py:153} INFO - Started process (PID=226) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:36:05,067] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:36:05,068] {logging_mixin.py:115} INFO - [2023-03-18 23:36:05,068] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:36:05,165] {logging_mixin.py:115} INFO - [2023-03-18 23:36:05,159] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:36:05,168] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:36:05,189] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.129 seconds
[2023-03-18 23:36:35,280] {processor.py:153} INFO - Started process (PID=231) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:36:35,281] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:36:35,284] {logging_mixin.py:115} INFO - [2023-03-18 23:36:35,284] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:36:35,369] {logging_mixin.py:115} INFO - [2023-03-18 23:36:35,365] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:36:35,371] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:36:35,393] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.118 seconds
[2023-03-18 23:37:05,485] {processor.py:153} INFO - Started process (PID=236) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:37:05,488] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:37:05,490] {logging_mixin.py:115} INFO - [2023-03-18 23:37:05,490] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:37:05,573] {logging_mixin.py:115} INFO - [2023-03-18 23:37:05,568] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:37:05,575] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:37:05,595] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 23:37:35,686] {processor.py:153} INFO - Started process (PID=241) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:37:35,689] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:37:35,691] {logging_mixin.py:115} INFO - [2023-03-18 23:37:35,691] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:37:35,778] {logging_mixin.py:115} INFO - [2023-03-18 23:37:35,773] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:37:35,781] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:37:35,803] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.122 seconds
[2023-03-18 23:38:05,900] {processor.py:153} INFO - Started process (PID=246) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:38:05,902] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:38:05,904] {logging_mixin.py:115} INFO - [2023-03-18 23:38:05,904] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:38:05,988] {logging_mixin.py:115} INFO - [2023-03-18 23:38:05,983] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:38:05,990] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:38:06,011] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.115 seconds
[2023-03-18 23:38:36,094] {processor.py:153} INFO - Started process (PID=251) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:38:36,097] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:38:36,098] {logging_mixin.py:115} INFO - [2023-03-18 23:38:36,098] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:38:36,179] {logging_mixin.py:115} INFO - [2023-03-18 23:38:36,175] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:38:36,181] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:38:36,203] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.113 seconds
[2023-03-18 23:39:06,292] {processor.py:153} INFO - Started process (PID=256) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:39:06,294] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:39:06,295] {logging_mixin.py:115} INFO - [2023-03-18 23:39:06,295] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:39:06,368] {logging_mixin.py:115} INFO - [2023-03-18 23:39:06,364] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:39:06,370] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:39:06,390] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.102 seconds
[2023-03-18 23:39:36,480] {processor.py:153} INFO - Started process (PID=261) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:39:36,484] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:39:36,486] {logging_mixin.py:115} INFO - [2023-03-18 23:39:36,486] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:39:36,574] {logging_mixin.py:115} INFO - [2023-03-18 23:39:36,568] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:39:36,576] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:39:36,598] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.123 seconds
[2023-03-18 23:40:06,690] {processor.py:153} INFO - Started process (PID=266) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:40:06,693] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:40:06,694] {logging_mixin.py:115} INFO - [2023-03-18 23:40:06,694] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:40:06,779] {logging_mixin.py:115} INFO - [2023-03-18 23:40:06,774] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:40:06,793] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:40:06,815] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.129 seconds
[2023-03-18 23:40:36,906] {processor.py:153} INFO - Started process (PID=271) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:40:36,908] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:40:36,910] {logging_mixin.py:115} INFO - [2023-03-18 23:40:36,910] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:40:37,008] {logging_mixin.py:115} INFO - [2023-03-18 23:40:37,003] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:40:37,011] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:40:37,033] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.131 seconds
[2023-03-18 23:41:07,125] {processor.py:153} INFO - Started process (PID=276) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:41:07,127] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:41:07,129] {logging_mixin.py:115} INFO - [2023-03-18 23:41:07,129] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:41:07,223] {logging_mixin.py:115} INFO - [2023-03-18 23:41:07,218] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:41:07,225] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:41:07,247] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.127 seconds
[2023-03-18 23:41:37,347] {processor.py:153} INFO - Started process (PID=281) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:41:37,349] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:41:37,351] {logging_mixin.py:115} INFO - [2023-03-18 23:41:37,350] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:41:37,439] {logging_mixin.py:115} INFO - [2023-03-18 23:41:37,435] {dagbag.py:320} ERROR - Failed to import: /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 317, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py", line 11, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 33, in <module>
    from google.cloud.bigquery import DEFAULT_RETRY
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/__init__.py", line 35, in <module>
    from google.cloud.bigquery.client import Client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 61, in <module>
    from google.cloud.bigquery_storage_v1.services.big_query_read.client import (
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/__init__.py", line 25, in <module>
    from google.cloud.bigquery_storage_v1 import client
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery_storage_v1/client.py", line 24, in <module>
    import google.api_core.gapic_v1.method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/__init__.py", line 18, in <module>
    from google.api_core.gapic_v1 import method
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 24, in <module>
    from google.api_core import grpc_helpers
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 29, in <module>
    import grpc_gcp
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/__init__.py", line 16, in <module>
    from grpc_gcp import _channel
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/_channel.py", line 19, in <module>
    from grpc_gcp.proto import grpc_gcp_pb2
  File "/home/airflow/.local/lib/python3.7/site-packages/grpc_gcp/proto/grpc_gcp_pb2.py", line 36, in <module>
    type=None),
  File "/home/airflow/.local/lib/python3.7/site-packages/google/protobuf/descriptor.py", line 796, in __new__
    _message.Message._CheckCalledFromGeneratedFile()
TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).

More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
[2023-03-18 23:41:37,441] {processor.py:653} WARNING - No viable dags retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:41:37,466] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.124 seconds
[2023-03-18 23:47:09,280] {processor.py:153} INFO - Started process (PID=180) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:47:09,284] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:47:09,293] {logging_mixin.py:115} INFO - [2023-03-18 23:47:09,293] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:47:17,709] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:47:18,591] {logging_mixin.py:115} INFO - [2023-03-18 23:47:18,590] {manager.py:508} INFO - Created Permission View: can delete on DAG:data_ingestion_gcs_dag
[2023-03-18 23:47:18,617] {logging_mixin.py:115} INFO - [2023-03-18 23:47:18,616] {manager.py:508} INFO - Created Permission View: can read on DAG:data_ingestion_gcs_dag
[2023-03-18 23:47:18,644] {logging_mixin.py:115} INFO - [2023-03-18 23:47:18,644] {manager.py:508} INFO - Created Permission View: can edit on DAG:data_ingestion_gcs_dag
[2023-03-18 23:47:18,671] {logging_mixin.py:115} INFO - [2023-03-18 23:47:18,662] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:47:18,703] {logging_mixin.py:115} INFO - [2023-03-18 23:47:18,702] {dag.py:2398} INFO - Creating ORM DAG for data_ingestion_gcs_dag
[2023-03-18 23:47:18,746] {logging_mixin.py:115} INFO - [2023-03-18 23:47:18,746] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00
[2023-03-18 23:47:18,772] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 9.539 seconds
[2023-03-18 23:47:50,003] {processor.py:153} INFO - Started process (PID=187) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:47:50,013] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:47:50,025] {logging_mixin.py:115} INFO - [2023-03-18 23:47:50,023] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:47:52,909] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:47:52,937] {logging_mixin.py:115} INFO - [2023-03-18 23:47:52,937] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:47:52,964] {logging_mixin.py:115} INFO - [2023-03-18 23:47:52,964] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00
[2023-03-18 23:47:52,977] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 3.108 seconds
[2023-03-18 23:48:23,104] {processor.py:153} INFO - Started process (PID=193) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:48:23,107] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:48:23,109] {logging_mixin.py:115} INFO - [2023-03-18 23:48:23,109] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:48:23,780] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:48:23,813] {logging_mixin.py:115} INFO - [2023-03-18 23:48:23,812] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:48:23,841] {logging_mixin.py:115} INFO - [2023-03-18 23:48:23,840] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-17T00:00:00+00:00, run_after=2023-03-18T00:00:00+00:00
[2023-03-18 23:48:23,857] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.758 seconds
[2023-03-18 23:48:53,960] {processor.py:153} INFO - Started process (PID=205) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:48:53,962] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:48:53,991] {logging_mixin.py:115} INFO - [2023-03-18 23:48:53,990] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:48:55,315] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:48:55,341] {logging_mixin.py:115} INFO - [2023-03-18 23:48:55,340] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:48:55,401] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 1.445 seconds
[2023-03-18 23:49:25,524] {processor.py:153} INFO - Started process (PID=229) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:49:25,527] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:49:25,528] {logging_mixin.py:115} INFO - [2023-03-18 23:49:25,528] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:49:26,101] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:49:26,122] {logging_mixin.py:115} INFO - [2023-03-18 23:49:26,122] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:49:26,142] {logging_mixin.py:115} INFO - [2023-03-18 23:49:26,142] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:49:26,155] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.635 seconds
[2023-03-18 23:49:56,316] {processor.py:153} INFO - Started process (PID=235) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:49:56,319] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:49:56,320] {logging_mixin.py:115} INFO - [2023-03-18 23:49:56,320] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:49:57,041] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:49:57,096] {logging_mixin.py:115} INFO - [2023-03-18 23:49:57,095] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:49:57,118] {logging_mixin.py:115} INFO - [2023-03-18 23:49:57,118] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:49:57,130] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.821 seconds
[2023-03-18 23:50:27,247] {processor.py:153} INFO - Started process (PID=241) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:50:27,249] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:50:27,251] {logging_mixin.py:115} INFO - [2023-03-18 23:50:27,250] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:50:27,736] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:50:27,760] {logging_mixin.py:115} INFO - [2023-03-18 23:50:27,759] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:50:27,783] {logging_mixin.py:115} INFO - [2023-03-18 23:50:27,783] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:50:27,796] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.553 seconds
[2023-03-18 23:50:57,910] {processor.py:153} INFO - Started process (PID=247) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:50:57,915] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:50:57,917] {logging_mixin.py:115} INFO - [2023-03-18 23:50:57,917] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:50:58,421] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:50:58,443] {logging_mixin.py:115} INFO - [2023-03-18 23:50:58,443] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:50:58,463] {logging_mixin.py:115} INFO - [2023-03-18 23:50:58,463] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:50:58,474] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.568 seconds
[2023-03-18 23:51:28,578] {processor.py:153} INFO - Started process (PID=253) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:51:28,579] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:51:28,581] {logging_mixin.py:115} INFO - [2023-03-18 23:51:28,581] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:51:29,069] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:51:29,094] {logging_mixin.py:115} INFO - [2023-03-18 23:51:29,094] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:51:29,115] {logging_mixin.py:115} INFO - [2023-03-18 23:51:29,115] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:51:29,127] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.554 seconds
[2023-03-18 23:51:59,246] {processor.py:153} INFO - Started process (PID=259) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:51:59,249] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:51:59,251] {logging_mixin.py:115} INFO - [2023-03-18 23:51:59,251] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:51:59,947] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:51:59,973] {logging_mixin.py:115} INFO - [2023-03-18 23:51:59,972] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:51:59,995] {logging_mixin.py:115} INFO - [2023-03-18 23:51:59,995] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:52:00,008] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.767 seconds
[2023-03-18 23:52:30,162] {processor.py:153} INFO - Started process (PID=265) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:52:30,166] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:52:30,167] {logging_mixin.py:115} INFO - [2023-03-18 23:52:30,167] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:52:30,740] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:52:30,763] {logging_mixin.py:115} INFO - [2023-03-18 23:52:30,763] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:52:30,791] {logging_mixin.py:115} INFO - [2023-03-18 23:52:30,791] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:52:30,821] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.663 seconds
[2023-03-18 23:53:00,941] {processor.py:153} INFO - Started process (PID=271) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:53:00,945] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:53:00,947] {logging_mixin.py:115} INFO - [2023-03-18 23:53:00,946] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:53:01,434] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:53:01,458] {logging_mixin.py:115} INFO - [2023-03-18 23:53:01,458] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:53:01,478] {logging_mixin.py:115} INFO - [2023-03-18 23:53:01,478] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:53:01,489] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.553 seconds
[2023-03-18 23:53:31,609] {processor.py:153} INFO - Started process (PID=277) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:53:31,612] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:53:31,613] {logging_mixin.py:115} INFO - [2023-03-18 23:53:31,613] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:53:32,121] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:53:32,144] {logging_mixin.py:115} INFO - [2023-03-18 23:53:32,143] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:53:32,165] {logging_mixin.py:115} INFO - [2023-03-18 23:53:32,164] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:53:32,175] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.571 seconds
[2023-03-18 23:54:02,282] {processor.py:153} INFO - Started process (PID=283) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:54:02,285] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:54:02,286] {logging_mixin.py:115} INFO - [2023-03-18 23:54:02,286] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:54:02,854] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:54:02,885] {logging_mixin.py:115} INFO - [2023-03-18 23:54:02,884] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:54:02,912] {logging_mixin.py:115} INFO - [2023-03-18 23:54:02,912] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:54:02,924] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.646 seconds
[2023-03-18 23:54:33,059] {processor.py:153} INFO - Started process (PID=289) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:54:33,061] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:54:33,063] {logging_mixin.py:115} INFO - [2023-03-18 23:54:33,062] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:54:33,579] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:54:33,601] {logging_mixin.py:115} INFO - [2023-03-18 23:54:33,601] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:54:33,623] {logging_mixin.py:115} INFO - [2023-03-18 23:54:33,623] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:54:33,634] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.579 seconds
[2023-03-18 23:55:03,748] {processor.py:153} INFO - Started process (PID=295) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:55:03,749] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:55:03,752] {logging_mixin.py:115} INFO - [2023-03-18 23:55:03,752] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:55:04,255] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:55:04,277] {logging_mixin.py:115} INFO - [2023-03-18 23:55:04,277] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:55:04,297] {logging_mixin.py:115} INFO - [2023-03-18 23:55:04,297] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:55:04,308] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.565 seconds
[2023-03-18 23:55:34,429] {processor.py:153} INFO - Started process (PID=301) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:55:34,431] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:55:34,433] {logging_mixin.py:115} INFO - [2023-03-18 23:55:34,432] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:55:34,933] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:55:34,954] {logging_mixin.py:115} INFO - [2023-03-18 23:55:34,954] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:55:34,973] {logging_mixin.py:115} INFO - [2023-03-18 23:55:34,973] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:55:34,984] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.560 seconds
[2023-03-18 23:56:05,106] {processor.py:153} INFO - Started process (PID=307) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:56:05,111] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:56:05,113] {logging_mixin.py:115} INFO - [2023-03-18 23:56:05,113] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:56:05,732] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:56:05,759] {logging_mixin.py:115} INFO - [2023-03-18 23:56:05,759] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:56:05,786] {logging_mixin.py:115} INFO - [2023-03-18 23:56:05,786] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:56:05,810] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.710 seconds
[2023-03-18 23:56:35,942] {processor.py:153} INFO - Started process (PID=313) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:56:35,948] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:56:35,949] {logging_mixin.py:115} INFO - [2023-03-18 23:56:35,949] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:56:36,448] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:56:36,472] {logging_mixin.py:115} INFO - [2023-03-18 23:56:36,471] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:56:36,491] {logging_mixin.py:115} INFO - [2023-03-18 23:56:36,491] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:56:36,504] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.566 seconds
[2023-03-18 23:57:06,614] {processor.py:153} INFO - Started process (PID=319) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:57:06,615] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:57:06,616] {logging_mixin.py:115} INFO - [2023-03-18 23:57:06,616] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:57:07,125] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:57:07,147] {logging_mixin.py:115} INFO - [2023-03-18 23:57:07,147] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:57:07,169] {logging_mixin.py:115} INFO - [2023-03-18 23:57:07,169] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:57:07,180] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.570 seconds
[2023-03-18 23:57:37,344] {processor.py:153} INFO - Started process (PID=325) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:57:37,346] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:57:37,347] {logging_mixin.py:115} INFO - [2023-03-18 23:57:37,347] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:57:37,838] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:57:37,861] {logging_mixin.py:115} INFO - [2023-03-18 23:57:37,860] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:57:37,881] {logging_mixin.py:115} INFO - [2023-03-18 23:57:37,881] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:57:37,892] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.552 seconds
[2023-03-18 23:58:08,007] {processor.py:153} INFO - Started process (PID=331) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:58:08,009] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:58:08,011] {logging_mixin.py:115} INFO - [2023-03-18 23:58:08,010] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:58:08,523] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:58:08,545] {logging_mixin.py:115} INFO - [2023-03-18 23:58:08,544] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:58:08,568] {logging_mixin.py:115} INFO - [2023-03-18 23:58:08,568] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:58:08,582] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.579 seconds
[2023-03-18 23:58:38,710] {processor.py:153} INFO - Started process (PID=337) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:58:38,711] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:58:38,713] {logging_mixin.py:115} INFO - [2023-03-18 23:58:38,713] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:58:39,210] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:58:39,236] {logging_mixin.py:115} INFO - [2023-03-18 23:58:39,235] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:58:39,255] {logging_mixin.py:115} INFO - [2023-03-18 23:58:39,255] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:58:39,267] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.562 seconds
[2023-03-18 23:59:09,378] {processor.py:153} INFO - Started process (PID=343) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:59:09,379] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:59:09,381] {logging_mixin.py:115} INFO - [2023-03-18 23:59:09,381] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:59:09,883] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:59:09,907] {logging_mixin.py:115} INFO - [2023-03-18 23:59:09,907] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:59:09,928] {logging_mixin.py:115} INFO - [2023-03-18 23:59:09,928] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:59:09,940] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.567 seconds
[2023-03-18 23:59:40,073] {processor.py:153} INFO - Started process (PID=349) to work on /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:59:40,074] {processor.py:641} INFO - Processing file /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py for tasks to queue
[2023-03-18 23:59:40,076] {logging_mixin.py:115} INFO - [2023-03-18 23:59:40,076] {dagbag.py:507} INFO - Filling up the DagBag from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:59:40,579] {processor.py:651} INFO - DAG(s) dict_keys(['data_ingestion_gcs_dag']) retrieved from /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py
[2023-03-18 23:59:40,601] {logging_mixin.py:115} INFO - [2023-03-18 23:59:40,601] {dag.py:2379} INFO - Sync 1 DAGs
[2023-03-18 23:59:40,622] {logging_mixin.py:115} INFO - [2023-03-18 23:59:40,622] {dag.py:2931} INFO - Setting next_dagrun for data_ingestion_gcs_dag to 2023-03-18T00:00:00+00:00, run_after=2023-03-19T00:00:00+00:00
[2023-03-18 23:59:40,633] {processor.py:161} INFO - Processing /Users/eugene/Personal_Projects/Data_Project/app/airflow/dags/data_ingestion_gcs.py took 0.565 seconds
